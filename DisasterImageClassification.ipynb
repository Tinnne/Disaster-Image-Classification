{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd0e7c72",
   "metadata": {},
   "source": [
    "# Disaster Image Classification\n",
    "\n",
    "Made By:\n",
    "- Austin Kane - 27022229232\n",
    "- Andreas Immanuel Lukito - 2702211595"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab04e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0, EfficientNetB2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "from PIL import ImageFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d81d830",
   "metadata": {},
   "source": [
    "## Set the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35c7b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224, 224) # MobileNetV2's input size is 224x224 \n",
    "BATCH_SIZE = 32 \n",
    "NUM_CLASSES = 4 # 4 classes of data\n",
    "DATA_DIR = 'Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8252b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255, # Normalize the images\n",
    "    validation_split=0.2, # Train Val Split\n",
    "    # Image Augmentation\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee52680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2754 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMAGE_SIZE, # Scale the image to fit model's input\n",
    "    batch_size=BATCH_SIZE, \n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221da1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 686 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMAGE_SIZE, # Scale the image to fit model's input\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e285e34",
   "metadata": {},
   "source": [
    "## MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d87ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(*IMAGE_SIZE, 3)\n",
    ")\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3bd606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "MobileNet = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1650ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_metric = F1Score(num_classes=NUM_CLASSES, average='macro')\n",
    "MobileNet.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[f1_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cfbe3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7130b7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - 86s 984ms/step - loss: 0.9726 - f1_score: 0.5642 - val_loss: 0.6394 - val_f1_score: 0.7276\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 83s 948ms/step - loss: 0.7064 - f1_score: 0.6871 - val_loss: 0.5855 - val_f1_score: 0.7517\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 78s 897ms/step - loss: 0.6452 - f1_score: 0.7198 - val_loss: 0.5953 - val_f1_score: 0.7396\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 76s 877ms/step - loss: 0.6227 - f1_score: 0.7296 - val_loss: 0.5254 - val_f1_score: 0.7597\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - 78s 891ms/step - loss: 0.5965 - f1_score: 0.7403 - val_loss: 0.5415 - val_f1_score: 0.7420\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - 73s 844ms/step - loss: 0.6078 - f1_score: 0.7496 - val_loss: 0.5590 - val_f1_score: 0.7523\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - 72s 823ms/step - loss: 0.5763 - f1_score: 0.7452 - val_loss: 0.5312 - val_f1_score: 0.7783\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - 72s 822ms/step - loss: 0.5605 - f1_score: 0.7530 - val_loss: 0.5479 - val_f1_score: 0.7597\n",
      "Epoch 9/10\n",
      "87/87 [==============================] - 73s 836ms/step - loss: 0.5347 - f1_score: 0.7775 - val_loss: 0.5118 - val_f1_score: 0.7545\n",
      "Epoch 10/10\n",
      "87/87 [==============================] - 73s 838ms/step - loss: 0.5462 - f1_score: 0.7687 - val_loss: 0.5285 - val_f1_score: 0.7685\n"
     ]
    }
   ],
   "source": [
    "history = MobileNet.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99f4ca5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 11s 486ms/step - loss: 0.4827 - f1_score: 0.8074\n",
      "Validation F1-Score: 0.8074\n"
     ]
    }
   ],
   "source": [
    "loss, f1 = MobileNet.evaluate(val_generator)\n",
    "print(f\"Validation F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0581d0",
   "metadata": {},
   "source": [
    "## EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc375800",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = EfficientNetB0(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(*IMAGE_SIZE, 3)\n",
    ")\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73fdb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "EffNetB0 = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b846218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EffNetB0.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[f1_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41c333d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - 93s 1s/step - loss: 1.2691 - f1_score: 0.3491 - val_loss: 1.2425 - val_f1_score: 0.1546\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 90s 1s/step - loss: 1.2468 - f1_score: 0.1583 - val_loss: 1.2424 - val_f1_score: 0.1546\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 89s 1s/step - loss: 1.2508 - f1_score: 0.1590 - val_loss: 1.2450 - val_f1_score: 0.1546\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 91s 1s/step - loss: 1.2528 - f1_score: 0.1554 - val_loss: 1.2475 - val_f1_score: 0.1546\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - 91s 1s/step - loss: 1.2522 - f1_score: 0.1568 - val_loss: 1.2422 - val_f1_score: 0.1546\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - 71s 813ms/step - loss: 1.2539 - f1_score: 0.1559 - val_loss: 1.2447 - val_f1_score: 0.1546\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - 73s 843ms/step - loss: 1.2548 - f1_score: 0.1543 - val_loss: 1.2446 - val_f1_score: 0.1546\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - 87s 996ms/step - loss: 1.2516 - f1_score: 0.1544 - val_loss: 1.2452 - val_f1_score: 0.1546\n",
      "Epoch 9/10\n",
      "87/87 [==============================] - 94s 1s/step - loss: 1.2520 - f1_score: 0.1544 - val_loss: 1.2420 - val_f1_score: 0.1546\n",
      "Epoch 10/10\n",
      "87/87 [==============================] - 92s 1s/step - loss: 1.2511 - f1_score: 0.1544 - val_loss: 1.2426 - val_f1_score: 0.1546\n"
     ]
    }
   ],
   "source": [
    "history = EffNetB0.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbf003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 19s 850ms/step - loss: 1.2427 - f1_score: 0.1546\n",
      "Validation Accuracy: 0.1546\n"
     ]
    }
   ],
   "source": [
    "loss, acc = EffNetB0.evaluate(val_generator)\n",
    "print(f\"Validation F1-Score: {acc:.4f}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4daec05",
   "metadata": {},
   "source": [
    "## EfficientNetB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7580e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (260, 260) # EfficientNetB2's input size is 224x224 \n",
    "BATCH_SIZE = 32 \n",
    "NUM_CLASSES = 4 # 4 classes of data\n",
    "DATA_DIR = 'Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27fd1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c431aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2754 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2788d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 686 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c522d977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
      "31793152/31790344 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = EfficientNetB2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(*IMAGE_SIZE, 3)\n",
    ")\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdc861e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "EffNetB2 = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c1953b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EffNetB2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "048c5051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - 97s 1s/step - loss: 1.2705 - accuracy: 0.4230 - val_loss: 1.2530 - val_accuracy: 0.4475\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 92s 1s/step - loss: 1.2543 - accuracy: 0.4423 - val_loss: 1.2438 - val_accuracy: 0.4475\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 89s 1s/step - loss: 1.2570 - accuracy: 0.4383 - val_loss: 1.2435 - val_accuracy: 0.4475\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 91s 1s/step - loss: 1.2575 - accuracy: 0.4466 - val_loss: 1.2442 - val_accuracy: 0.4475\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - 92s 1s/step - loss: 1.2515 - accuracy: 0.4466 - val_loss: 1.2443 - val_accuracy: 0.4475\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - 89s 1s/step - loss: 1.2541 - accuracy: 0.4466 - val_loss: 1.2453 - val_accuracy: 0.4475\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - 89s 1s/step - loss: 1.2534 - accuracy: 0.4466 - val_loss: 1.2506 - val_accuracy: 0.4475\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - 91s 1s/step - loss: 1.2567 - accuracy: 0.4466 - val_loss: 1.2419 - val_accuracy: 0.4475\n",
      "Epoch 9/10\n",
      "87/87 [==============================] - 88s 1s/step - loss: 1.2505 - accuracy: 0.4466 - val_loss: 1.2428 - val_accuracy: 0.4475\n",
      "Epoch 10/10\n",
      "87/87 [==============================] - 93s 1s/step - loss: 1.2515 - accuracy: 0.4466 - val_loss: 1.2422 - val_accuracy: 0.4475\n"
     ]
    }
   ],
   "source": [
    "history = EffNetB2.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "200234b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 18s 797ms/step - loss: 1.2423 - accuracy: 0.4475\n",
      "Validation Accuracy: 0.4475\n"
     ]
    }
   ],
   "source": [
    "loss, acc = EffNetB2.evaluate(val_generator)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
